{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "from train_utils import plot_confusion_matrix, make_dataframe\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "print(\"Tensorflow version is \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width_image = 224\n",
    "height_image = 224\n",
    "batch_size = 32\n",
    "img_shape = (width_image, height_image, 3)\n",
    "\n",
    "categories = os.listdir('dataset\\\\v2')\n",
    "data_frames = make_dataframe('dataset\\\\v2', validation_percentage = 0.13, test_percentage=0.17)\n",
    "\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=15.\n",
    ")\n",
    "validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make generator\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=data_frames[0],\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    class_mode = 'categorical',\n",
    "    target_size = (width_image, height_image),\n",
    "    batch_size = batch_size)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=data_frames[1],\n",
    "    x_col = 'path',\n",
    "    y_col = 'label',\n",
    "    class_mode = 'categorical',\n",
    "    target_size=(width_image, height_image),\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=data_frames[2],\n",
    "    x_col = 'path',\n",
    "    y_col = 'label',\n",
    "    class_mode = 'categorical',\n",
    "    target_size=(width_image, height_image),\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-trained MobileNet V2\n",
    "#base_model = tf.keras.applications.MobileNetV2(input_shape=img_shape,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "#base_model = tf.keras.applications.InceptionResNetV2(input_shape=img_shape,\n",
    "#                                                include_top=False,\n",
    "#                                                weights='imagenet')\n",
    "\n",
    "base_model = tf.keras.applications.densenet.DenseNet121(input_shape=img_shape,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "\n",
    "# feature extraction reuse, True for fine-tuning see tensorflow\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model over pre-trained net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model over pre-trained graph\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "#    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                       activation=tf.nn.relu, input_shape=(len(categories),)),\n",
    "#    keras.layers.Dropout(0.5),\n",
    "#    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "#                       activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.GlobalAveragePooling2D(),\n",
    "    keras.layers.Dense(len(categories), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the optmizer, loss function and metric for evaluate the training\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# optimizer = keras.optimizers.Nadam()\n",
    "# optimizer = keras.optimizers.Adadelta()\n",
    "# tensorflow guide use:\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranining Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "test_steps = test_generator.n // batch_size\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = epochs, \n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable chage of all net weight's including the pre-trained\n",
    "epochs_fine = 100\n",
    "model.trainable = True\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the level to fine-tune\n",
    "fine_tune_at = 200\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompile model with optimizer\n",
    "optimizer = keras.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "#optimizer = tf.keras.optimizers.RMSprop(lr=2e-5)\n",
    "\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit fine-tuning\n",
    "old_fine = history_fine\n",
    "history_fine = model.fit_generator(train_generator,\n",
    "                                   steps_per_epoch = steps_per_epoch,\n",
    "                                   epochs = epochs_fine,\n",
    "                                   validation_data=validation_generator,\n",
    "                                   validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Validation accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# training and validation accuracy/loss\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "if('history_fine' in globals()):\n",
    "    acc += history_fine.history['acc']\n",
    "    val_acc += history_fine.history['val_acc']\n",
    "    loss += history_fine.history['loss']\n",
    "    val_loss += history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "Y_pred = model.predict_generator(test_generator, test_steps)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "\n",
    "# classification report\n",
    "print('Classification Report')\n",
    "print(classification_report(test_generator.classes, y_pred, target_names=categories))\n",
    "\n",
    "# evaluate keras model with model.evaluate()\n",
    "x, y = zip(*(test_generator[i] for i in range(len(test_generator))))\n",
    "x_test, y_test = np.vstack(x), np.vstack(y)\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "print(\"Accuracy: {0:0.1f}%\".format(acc * 100))\n",
    "print(\"Loss: {0:0.1f}%\".format(loss * 100))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(test_generator.classes, y_pred, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "name = 'adam_densnet121.h5'\n",
    "model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_extractor = tf.keras.applications.densenet.DenseNet121(\n",
    "    input_shape=img_shape,\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model over pre-trained graph, its same to pass pooling='avg'\n",
    "#model_extractor = tf.keras.Sequential([\n",
    "#   base_model_extractor\n",
    "#    keras.layers.GlobalAveragePooling2D()\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    " \n",
    "def train_svm_classifer(features, labels, model_output_path):\n",
    "    \"\"\"\n",
    "    train_svm_classifer will train a SVM, saved the trained and SVM model and\n",
    "    report the classification performance\n",
    " \n",
    "    features: array of input features\n",
    "    labels: array of labels associated with the input features\n",
    "    model_output_path: path for storing the trained svm model\n",
    "    \"\"\"\n",
    "    # save 20% of data for performance evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    " \n",
    "    param = [\n",
    "        {\n",
    "            \"kernel\": [\"linear\"],\n",
    "            \"C\": [1, 10, 100, 1000]\n",
    "        },\n",
    "        {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [1, 10, 100, 1000],\n",
    "            \"gamma\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        }\n",
    "    ]\n",
    " \n",
    "    # request probability estimation\n",
    "    svm = SVC(probability=True)\n",
    " \n",
    "    # 10-fold cross validation, use 4 thread as each fold and each parameter set can be train in parallel\n",
    "    clf = GridSearchCV(svm, param,\n",
    "            cv=10, n_jobs=4, verbose=3)\n",
    " \n",
    "    clf.fit(X_train, y_train)\n",
    " \n",
    "    if os.path.exists(model_output_path):\n",
    "        joblib.dump(clf.best_estimator_, model_output_path)\n",
    "    else:\n",
    "        print(\"Cannot save trained svm model to {0}.\".format(model_output_path))\n",
    " \n",
    "    print(\"\\nBest parameters set:\")\n",
    "    print(clf.best_params_)\n",
    " \n",
    "    y_predict=clf.predict(X_test)\n",
    " \n",
    "    labels=sorted(list(set(labels)))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print(\"Labels: {0}\\n\".format(\",\".join(labels)))\n",
    "    print(confusion_matrix(y_test, y_predict, labels=labels))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "def extract_features(generator, model, size):\n",
    "    features = np.empty((size, 1024))\n",
    "    labels = []\n",
    "    for i in range(0, size):\n",
    "        img, label = generator.next()\n",
    "        feature = model.predict(img)\n",
    "        features[i,:] = np.squeeze(feature)\n",
    "        labels.append(categories[np.argmax(label)])\n",
    "        print(\"{0:.0f}%\".format((i/size) * 100), end=\"\\r\")\n",
    "    return features, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=data_frames[0],\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    class_mode = 'categorical',\n",
    "    target_size = (width_image, height_image),\n",
    "    batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = extract_features(train_svm_generator, base_model_extractor, size=len(train_svm_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm_classifer(features, labels, 'aa.bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
